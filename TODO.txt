
So the key thing to note is that the file name is the id of the document, i.e. DOCID,

so the schema you set up, ie. should put the filename (without extension) as the docid, then you can extract out the title (if the page has one),

if you can extract the base url, then put that into source. Don’t worry about the date and time.

For the content field,  you have a number of options: (1) dump in all the text in the html page.
or (2) parse the page and extract out the important blocks of text, ie. the useful information, not all the guff (headers/footers, ads, etc).

Do (1) first, then you can try something like : http://infolab.stanford.edu/~prasen9/sac05.pdf
it is likely that someone has already implemented similar algos.


Break the indexing task down:
- create test sample of  5 documents
- extract out each field manually.
- write functions to extract out these fields.
- test whether the manually extracted text matches the automatically extracted test
	- write some test units for this

Filtering out content.
- identify some papers in the literature that extract the body text / main content of a html page
- identify some toolkits that are online to do this.
- write a page or two to describe them.
- which one(s) should you try out?
- try out a toolkit/or write your own parser, etc.

Basic Searcher
- write a console (command line) application that asks the user for query then returns the top ten results.
	- make sure your write a series of functions (not all one amorphous script)


- write another console application, that takes an index file path and a query file reference.
	- load the index
	- read in the query file,
		- format:
			queryid querystring
		- example
			Q1 sharp pains in the side of my head
			Q2 throbbing headache that won’t go away
			Q3 etc
	- for each query,
		run it agains the index
		output for the first n=10 queries.
		queryid, docid, rank and score


def get_html(html_file):
    with codecs.open(path + html_file, 'r', "utf-8") as f:
        html = f.read()
    return html

